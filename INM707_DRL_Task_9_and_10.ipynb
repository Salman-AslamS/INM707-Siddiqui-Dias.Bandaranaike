{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!apt install xvfb\n",
        "!pip install ale-py\n",
        "# !pip install atari-py\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "# !apt install xvfb\n",
        "# !pip install gym[atari]\n",
        "!pip install gym-notebook-wrapper\n",
        "!pip install atari-py\n",
        "!pip install pyvirtualdisplay\n",
        "\n"
      ],
      "metadata": {
        "id": "1jMVYsrvQe0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# %%bash\n",
        "\n",
        "# curl -O http://www.atarimania.com/roms/Roms.rar\n",
        "# mkdir roms\n",
        "# yes | unrar e Roms.rar roms/\n",
        "# python -m atari_py.import_roms roms/\n",
        "# %%bash\n",
        "\n"
      ],
      "metadata": {
        "id": "j66dcT3dp-Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rm -rf game/*\n",
        "# mkdir -p game\n",
        "!pip install ray[rllib]==1.13.0\n",
        "# !pip install -U \"ray[train]\"\n",
        "!pip install -U \"ray[train]\""
      ],
      "metadata": {
        "id": "lcy3sfRJqAFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lz4"
      ],
      "metadata": {
        "id": "ipgspUURrmFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.tune.registry import register_env\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "import os\n",
        "from ray import air\n",
        "import torch.nn.functional as F\n",
        "from ray.rllib.utils.replay_buffers import ReplayBuffer\n",
        "\n",
        "sys.setrecursionlimit(10000)  # Increase the recursion limit\n",
        "\n",
        "# https://www.bing.com/search?pglt=43&q=nn.Module&cvid=3e8744507f7d432fb59a2a95abeccad8&gs_lcrp=\n",
        "# EgZjaHJvbWUyBggAEEUYOTIGCAEQABhAMgYIAhAAGEAyBggDEAAYQDIGCAQQABhAMgYIBRAAGEAyBggGEAAYQDIGCAcQABhAMgYICBBFGDzSAQcxNDhqMGoxqAIAsAIA&FORM=ANNTA1&PC=DCTS\n",
        "\n",
        "# https://docs.ray.io/en/latest/tune/api/doc/ray.tune.ResultGrid.get_best_result.html\n",
        "# https://docs.ray.io/en/latest/tune/tutorials/tune-trial-checkpoints.html\n",
        "# https://docs.ray.io/en/latest/rllib/rllib-saving-and-loading-algos-and-policies.html\n",
        "\n",
        "# Define the DDQN model\n",
        "class DDQNModel(TorchModelV2, nn.Module):\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
        "        super().__init__(obs_space, action_space, num_outputs, model_config, name)\n",
        "        nn.Module.__init__(self)\n",
        "        self.num_actions = action_space.n\n",
        "\n",
        "        self.conv1 = nn.Conv2d(obs_space.shape[0], 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.fc1 = nn.Linear(self.feature_size(obs_space.shape), 512)\n",
        "        self.fc2 = nn.Linear(512, self.num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def feature_size(self, shape):\n",
        "        conv1_shape = self.conv_output_shape(shape, self.conv1)\n",
        "        conv2_shape = self.conv_output_shape(conv1_shape, self.conv2)\n",
        "        return int(np.prod(conv2_shape))\n",
        "\n",
        "    def conv_output_shape(self, shape, conv):\n",
        "        output_shape = [\n",
        "            (shape[i] - conv.kernel_size[i] + 2 * conv.padding[i]) // conv.stride[i] + 1\n",
        "            for i in range(len(shape) - 1)\n",
        "        ]\n",
        "        return [conv.out_channels] + output_shape\n",
        "\n",
        "# Register the custom model\n",
        "ModelCatalog.register_custom_model(\"ddqn_model\", DDQNModel)\n",
        "\n",
        "env_name = \"ALE/BankHeist-v5\"  # Using the BankHeist-v5 environment\n",
        "\n",
        "def env_creator(env_config):\n",
        "    env = gym.make(env_name, new_step_api=True)\n",
        "\n",
        "    original_reset = env.reset\n",
        "\n",
        "    def reset_wrapper(return_info=False):\n",
        "        obs, info = original_reset(return_info=return_info)\n",
        "        return (np.array(obs), info) if return_info else np.array(obs)\n",
        "\n",
        "    env.reset = reset_wrapper\n",
        "    return env\n",
        "\n",
        "class DDQNTrainable(tune.Trainable):\n",
        "    def setup(self, config):\n",
        "        self.env = env_creator(config[\"env_config\"])\n",
        "        self.config = config\n",
        "        self.model = DDQNModel(\n",
        "            self.env.observation_space,\n",
        "            self.env.action_space,\n",
        "            self.env.action_space.n,\n",
        "            config[\"model\"],\n",
        "            \"ddqn_model\",\n",
        "        )\n",
        "\n",
        "def compute_target_q_values(self, batch):\n",
        "    \"\"\"\n",
        "    Compute the target Q-values for a batch of transitions.\n",
        "\n",
        "    Args:\n",
        "        batch: A tuple or list containing the batch of transitions (state, action, reward, next_state, done).\n",
        "\n",
        "    Returns:\n",
        "        A numpy array of target Q-values for each transition in the batch.\n",
        "    \"\"\"\n",
        "    states, actions, rewards, next_states, dones = batch\n",
        "\n",
        "    # Computing the Q-values for the next states using the target network\n",
        "    next_q_values = self.model.target(next_states).detach().max(1)[0]\n",
        "\n",
        "    # Compute the target Q-values using the Bellman equation\n",
        "    target_q_values = rewards + self.config[\"gamma\"] * next_q_values * (1 - dones)\n",
        "\n",
        "    return target_q_values.numpy()\n",
        "\n",
        "def update_target_network(self):\n",
        "    \"\"\"\n",
        "    Update the target network weights by copying the weights from the current model.\n",
        "    \"\"\"\n",
        "    self.model.target.load_state_dict(self.model.state_dict())\n",
        "\n",
        "def train_on_batch(self, batch, target_q_values):\n",
        "    \"\"\"\n",
        "    Perform a training step on a batch of transitions.\n",
        "\n",
        "    Args:\n",
        "        batch: A tuple or list containing the batch of transitions (state, action, reward, next_state, done).\n",
        "        target_q_values: A numpy array of target Q-values for each transition in the batch.\n",
        "\n",
        "    Returns:\n",
        "        The loss computed for the current batch.\n",
        "    \"\"\"\n",
        "    states, actions, _, _, _ = batch\n",
        "\n",
        "    # Computing the Q-values for the current states using the model\n",
        "    q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Computing the loss using the Huber loss function\n",
        "    loss = F.smooth_l1_loss(q_values, torch.from_numpy(target_q_values))\n",
        "\n",
        "    # Optimizing the model\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def step(self):\n",
        "    # Collect experiences and store them in the replay buffer\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    obs = self.env.reset()\n",
        "    replay_buffer = ReplayBuffer(self.config[\"replay_buffer_config\"])\n",
        "\n",
        "    while not done:\n",
        "        # Choose an action\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(torch.from_numpy(np.array(obs)).float().unsqueeze(0))\n",
        "            action = q_values.argmax().item()\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Store the transition in the replay buffer\n",
        "        replay_buffer.add(obs, action, reward, next_obs, done)\n",
        "\n",
        "        obs = next_obs\n",
        "\n",
        "    # Update the model if enough samples are available in the replay buffer\n",
        "    if len(replay_buffer) >= self.config[\"train_batch_size\"]:\n",
        "        # Sample a batch of transitions from the replay buffer\n",
        "        batch = replay_buffer.sample(self.config[\"train_batch_size\"])\n",
        "\n",
        "        # Compute the target Q-values\n",
        "        target_q_values = self.compute_target_q_values(batch)\n",
        "\n",
        "        # Perform a training step\n",
        "        loss = self.train_on_batch(batch, target_q_values)\n",
        "\n",
        "        # Update the target network periodically\n",
        "        if self.iteration % self.config[\"target_network_update_freq\"] == 0:\n",
        "            self.update_target_network()\n",
        "\n",
        "    return {\"episode_reward_mean\": episode_reward}\n",
        "\n",
        "def save_checkpoint(self, tmp_checkpoint_dir):\n",
        "    checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
        "    torch.save(self.model.state_dict(), checkpoint_path)\n",
        "    return tmp_checkpoint_dir\n",
        "\n",
        "def load_checkpoint(self, tmp_checkpoint_dir):\n",
        "    checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
        "    self.model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "def get_config():\n",
        "    config = DQNConfig()\n",
        "    config = config.training(\n",
        "        double_q=True,\n",
        "        target_network_update_freq=8000,\n",
        "        replay_buffer_config={\n",
        "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
        "            \"capacity\": 60000,\n",
        "            \"prioritized_replay_alpha\": 0.5,\n",
        "            \"prioritized_replay_beta\": 0.5,\n",
        "            \"prioritized_replay_eps\": 3e-6,\n",
        "        },\n",
        "        lr_schedule=tune.grid_search([[0, 0.0000625], [1000000, 0.000006]]),\n",
        "        adam_epsilon=0.00015,\n",
        "        grad_clip=10,\n",
        "        hiddens=[256],\n",
        "    )\n",
        "    config = config.resources(num_gpus=0)\n",
        "    config = config.env_runners(num_env_runners=1, rollout_fragment_length=4, batch_mode=\"complete_episodes\")\n",
        "\n",
        "    # Define the observation space and action space explicitly\n",
        "    observation_space = gym.spaces.Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)\n",
        "    action_space = env_creator(None).action_space\n",
        "\n",
        "    config = config.environment(env_name, observation_space=observation_space, action_space=action_space)\n",
        "\n",
        "    config.model = {\n",
        "        \"fcnet_hiddens\": tune.grid_search([[64, 64], [128, 128]]),\n",
        "        \"fcnet_activation\": \"relu\",\n",
        "        \"conv_filters\": [[16, 8, 4], [32, 4, 2], [64, 3, 1]],\n",
        "        \"conv_activation\": \"relu\",\n",
        "        \"custom_model_config\": {\n",
        "            \"encoder_latent_dim\": 128\n",
        "        },\n",
        "        \"custom_model\": \"ddqn_model\",\n",
        "    }\n",
        "\n",
        "    return config\n",
        "\n",
        "# Start the Tune run\n",
        "tuner = tune.Tuner(\n",
        "    DDQNTrainable,\n",
        "    param_space=get_config().to_dict(),\n",
        "    run_config=air.RunConfig(\n",
        "        stop={\"training_iteration\": 500},\n",
        "        checkpoint_config=air.CheckpointConfig(checkpoint_frequency=10),\n",
        "    ),\n",
        "    tune_config=tune.TuneConfig(metric=\"episode_reward_mean\", mode=\"max\"),\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "# Get the best result from the tuning process\n",
        "best_result = results.get_best_result(metric=\"episode_reward_mean\", scope='avg', filter_nan_and_inf=False)\n",
        "\n",
        "# Restore the best model checkpoint\n",
        "if best_result:\n",
        "    best_checkpoint = best_result.checkpoint\n",
        "    best_trainable = DDQNTrainable(best_result.config)\n",
        "    best_trainable.restore(best_checkpoint)\n",
        "\n",
        "    # Evaluate the trained agent\n",
        "    env = env_creator(best_result.config[\"env_config\"])\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = best_trainable.model.compute_actions(obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "    print(\"Total Reward:\", total_reward)\n",
        "    env.close()\n",
        "else:\n",
        "    print(\"No valid checkpoint found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17109
        },
        "id": "BbD2phoR33bv",
        "outputId": "b1aaf683-bdc0-434c-b4e0-6adb2918f18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-12 13:20:52,311\tWARNING tune.py:885 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, wrap `train_func` with `tune.with_resources(train_func, resources_per_trial={'gpu': 1})` which allows Tune to expose 1 GPU to each trial. For Ray Train Trainers, you can specify GPU resources through `ScalingConfig(use_gpu=True)`. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+\n",
            "| Configuration for experiment     DDQNTrainable_2024-05-12_13-20-52   |\n",
            "+----------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator               |\n",
            "| Scheduler                        FIFOScheduler                       |\n",
            "| Number of trials                 4                                   |\n",
            "+----------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/DDQNTrainable_2024-05-12_13-20-52\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts`\n",
            "\n",
            "Trial status: 4 PENDING\n",
            "Current time: 2024-05-12 13:20:52. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs\n",
            "+--------------------------------------------------------------------------------------------------+\n",
            "| Trial name                                   status     lr_schedule        model/fcnet_hiddens   |\n",
            "+--------------------------------------------------------------------------------------------------+\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00000   PENDING    [0, 6.25e-05]      [64, 64]              |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00001   PENDING    [1000000, 6e-06]   [64, 64]              |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00002   PENDING    [0, 6.25e-05]      [128, 128]            |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00003   PENDING    [1000000, 6e-06]   [128, 128]            |\n",
            "+--------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=26707)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[36m(pid=26707)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[36m(DDQNTrainable pid=26707)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "\u001b[36m(DDQNTrainable pid=26707)\u001b[0m [Powered by Stella]\n",
            "\u001b[36m(DDQNTrainable pid=26707)\u001b[0m Install gputil for GPU system monitoring.\n",
            "2024-05-12 13:21:12,596\tERROR tune_controller.py:1331 -- Trial task failed for trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00001\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2623, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(NotImplementedError): \u001b[36mray::DDQNTrainable.train()\u001b[39m (pid=26707, ip=172.28.0.12, actor_id=ad79b657d3be7bd98bafe39701000000, repr=<__main__.DDQNTrainable object at 0x79ae58e8f4c0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 328, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 850, in step\n",
            "    raise NotImplementedError\n",
            "NotImplementedError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00001 started with configuration:\n",
            "+----------------------------------------------------------------------------------+\n",
            "| Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00001 config                          |\n",
            "+----------------------------------------------------------------------------------+\n",
            "| _AlgorithmConfig__prior_exploration_config                                       |\n",
            "| _disable_action_flattening                                                 False |\n",
            "| _disable_execution_plan_api                                                   -1 |\n",
            "| _disable_initialize_loss_from_dummy_batch                                  False |\n",
            "| _disable_preprocessor_api                                                  False |\n",
            "| _enable_rl_module_api                                                         -1 |\n",
            "| _env_to_module_connector                                                         |\n",
            "| _evaluation_parallel_to_training_wo_thread                                 False |\n",
            "| _fake_gpus                                                                 False |\n",
            "| _is_atari                                                                        |\n",
            "| _learner_class                                                                   |\n",
            "| _learner_connector                                                               |\n",
            "| _module_to_env_connector                                                         |\n",
            "| _rl_module_spec                                                                  |\n",
            "| _run_training_always_in_thread                                             False |\n",
            "| _tf_policy_handles_more_than_one_loss                                      False |\n",
            "| action_mask_key                                                      action_mask |\n",
            "| action_space                                                        Discrete(18) |\n",
            "| actions_in_input_normalized                                                False |\n",
            "| adam_epsilon                                                             0.00015 |\n",
            "| add_default_connectors_to_env_to_module_pipeline                            True |\n",
            "| add_default_connectors_to_learner_pipeline                                  True |\n",
            "| add_default_connectors_to_module_to_env_pipeline                            True |\n",
            "| always_attach_evaluation_results                                            True |\n",
            "| auto_wrap_old_gym_envs                                                        -1 |\n",
            "| batch_mode                                                     complete_episodes |\n",
            "| before_learn_on_batch                                                            |\n",
            "| callbacks                                                   ...efaultCallbacks'> |\n",
            "| categorical_distribution_temperature                                          1. |\n",
            "| checkpoint_trainable_policies_only                                         False |\n",
            "| clip_actions                                                               False |\n",
            "| clip_rewards                                                                     |\n",
            "| compress_observations                                                      False |\n",
            "| count_steps_by                                                         env_steps |\n",
            "| create_env_on_driver                                                       False |\n",
            "| custom_async_evaluation_function                                              -1 |\n",
            "| custom_eval_function                                                             |\n",
            "| delay_between_env_runner_restarts_s                                          60. |\n",
            "| disable_env_checking                                                          -1 |\n",
            "| double_q                                                                    True |\n",
            "| dueling                                                                     True |\n",
            "| eager_max_retraces                                                            20 |\n",
            "| eager_tracing                                                               True |\n",
            "| enable_async_evaluation                                                       -1 |\n",
            "| enable_connectors                                                           True |\n",
            "| enable_env_runner_and_connector_v2                                         False |\n",
            "| enable_rl_module_and_learner                                               False |\n",
            "| enable_tf1_exec_eagerly                                                    False |\n",
            "| env                                                             ALE/BankHeist-v5 |\n",
            "| env_runner_cls                                                                   |\n",
            "| env_runner_health_probe_timeout_s                                             30 |\n",
            "| env_runner_restore_timeout_s                                                1800 |\n",
            "| env_task_fn                                                                      |\n",
            "| episode_lookback_horizon                                                       1 |\n",
            "| epsilon                                                     ...), (10000, 0.05)] |\n",
            "| evaluation_config/explore                                                  False |\n",
            "| evaluation_duration                                                           10 |\n",
            "| evaluation_duration_unit                                                episodes |\n",
            "| evaluation_force_reset_envs_before_iteration                                True |\n",
            "| evaluation_interval                                                              |\n",
            "| evaluation_num_env_runners                                                     0 |\n",
            "| evaluation_parallel_to_training                                            False |\n",
            "| evaluation_sample_timeout_s                                                 120. |\n",
            "| exploration_config/epsilon_timesteps                                       10000 |\n",
            "| exploration_config/final_epsilon                                            0.02 |\n",
            "| exploration_config/initial_epsilon                                           1.0 |\n",
            "| exploration_config/type                                            EpsilonGreedy |\n",
            "| explore                                                                     True |\n",
            "| export_native_model_files                                                  False |\n",
            "| fake_sampler                                                               False |\n",
            "| framework                                                                  torch |\n",
            "| gamma                                                                       0.99 |\n",
            "| grad_clip                                                                     10 |\n",
            "| grad_clip_by                                                         global_norm |\n",
            "| hiddens                                                                    [256] |\n",
            "| ignore_env_runner_failures                                                 False |\n",
            "| in_evaluation                                                              False |\n",
            "| input                                                                    sampler |\n",
            "| keep_per_episode_custom_metrics                                            False |\n",
            "| local_gpu_idx                                                                  0 |\n",
            "| local_tf_session_args/inter_op_parallelism_threads                             8 |\n",
            "| local_tf_session_args/intra_op_parallelism_threads                             8 |\n",
            "| log_level                                                                   WARN |\n",
            "| log_sys_usage                                                               True |\n",
            "| logger_config                                                                    |\n",
            "| logger_creator                                                                   |\n",
            "| lr                                                                        0.0005 |\n",
            "| lr_schedule                                                     [1000000, 6e-06] |\n",
            "| max_num_env_runner_restarts                                                 1000 |\n",
            "| max_requests_in_flight_per_sampler_worker                                      2 |\n",
            "| metrics_episode_collection_timeout_s                                         60. |\n",
            "| metrics_num_episodes_for_smoothing                                           100 |\n",
            "| min_sample_timesteps_per_iteration                                          1000 |\n",
            "| min_time_s_per_iteration                                                         |\n",
            "| min_train_timesteps_per_iteration                                              0 |\n",
            "| model/conv_activation                                                       relu |\n",
            "| model/conv_filters                                          ..., 2], [64, 3, 1]] |\n",
            "| model/custom_model                                                    ddqn_model |\n",
            "| model/custom_model_config/encoder_latent_dim                                 128 |\n",
            "| model/fcnet_activation                                                      relu |\n",
            "| model/fcnet_hiddens                                                     [64, 64] |\n",
            "| n_step                                                                         1 |\n",
            "| noisy                                                                      False |\n",
            "| normalize_actions                                                           True |\n",
            "| num_atoms                                                                      1 |\n",
            "| num_consecutive_env_runner_failures_tolerance                                100 |\n",
            "| num_cpus_for_driver                                                            1 |\n",
            "| num_cpus_per_learner_worker                                                    1 |\n",
            "| num_cpus_per_worker                                                            1 |\n",
            "| num_envs_per_env_runner                                                        1 |\n",
            "| num_gpus                                                                       0 |\n",
            "| num_gpus_per_learner_worker                                                    0 |\n",
            "| num_gpus_per_worker                                                            0 |\n",
            "| num_learner_workers                                                            0 |\n",
            "| num_steps_sampled_before_learning_starts                                    1000 |\n",
            "| num_workers                                                                    1 |\n",
            "| observation_filter                                                      NoFilter |\n",
            "| observation_fn                                                                   |\n",
            "| observation_space                                           ..., 160, 3), uint8) |\n",
            "| offline_sampling                                                           False |\n",
            "| ope_split_batch_by_episode                                                  True |\n",
            "| output                                                                           |\n",
            "| output_compress_columns                                       ['obs', 'new_obs'] |\n",
            "| output_max_file_size                                                    67108864 |\n",
            "| placement_strategy                                                          PACK |\n",
            "| policies/default_policy                                     ...None, None, None) |\n",
            "| policies_to_train                                                                |\n",
            "| policy_map_cache                                                              -1 |\n",
            "| policy_map_capacity                                                          100 |\n",
            "| policy_mapping_fn                                           ...t 0x7baee9b49e10> |\n",
            "| policy_states_are_swappable                                                False |\n",
            "| postprocess_inputs                                                         False |\n",
            "| preprocessor_pref                                                       deepmind |\n",
            "| recreate_failed_env_runners                                                False |\n",
            "| remote_env_batch_wait_ms                                                       0 |\n",
            "| remote_worker_envs                                                         False |\n",
            "| render_env                                                                 False |\n",
            "| replay_buffer_config/capacity                                              60000 |\n",
            "| replay_buffer_config/prioritized_replay                                       -1 |\n",
            "| replay_buffer_config/prioritized_replay_alpha                                0.5 |\n",
            "| replay_buffer_config/prioritized_replay_beta                                 0.5 |\n",
            "| replay_buffer_config/prioritized_replay_eps                                3e-06 |\n",
            "| replay_buffer_config/replay_sequence_length                                    1 |\n",
            "| replay_buffer_config/type                                   ...tizedReplayBuffer |\n",
            "| replay_buffer_config/worker_side_prioritization                            False |\n",
            "| replay_sequence_length                                                           |\n",
            "| restart_failed_sub_environments                                            False |\n",
            "| rollout_fragment_length                                                        4 |\n",
            "| sample_collector                                            ...leListCollector'> |\n",
            "| sample_timeout_s                                                             60. |\n",
            "| sampler_perf_stats_ema_coef                                                      |\n",
            "| seed                                                                             |\n",
            "| shuffle_buffer_size                                                            0 |\n",
            "| sigma0                                                                       0.5 |\n",
            "| simple_optimizer                                                              -1 |\n",
            "| store_buffer_in_checkpoints                                                False |\n",
            "| sync_filters_on_rollout_workers_timeout_s                                    10. |\n",
            "| synchronize_filters                                                           -1 |\n",
            "| target_network_update_freq                                                  8000 |\n",
            "| tau                                                                           1. |\n",
            "| td_error_loss_fn                                                           huber |\n",
            "| tf_session_args/allow_soft_placement                                        True |\n",
            "| tf_session_args/device_count/CPU                                               1 |\n",
            "| tf_session_args/gpu_options/allow_growth                                    True |\n",
            "| tf_session_args/inter_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/intra_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/log_device_placement                                       False |\n",
            "| torch_compile_learner                                                      False |\n",
            "| torch_compile_learner_dynamo_backend                                    inductor |\n",
            "| torch_compile_learner_dynamo_mode                                                |\n",
            "| torch_compile_learner_what_to_compile                       ...ile.FORWARD_TRAIN |\n",
            "| torch_compile_worker                                                       False |\n",
            "| torch_compile_worker_dynamo_backend                                       onnxrt |\n",
            "| torch_compile_worker_dynamo_mode                                                 |\n",
            "| train_batch_size                                                              32 |\n",
            "| train_batch_size_per_learner                                                     |\n",
            "| training_intensity                                                               |\n",
            "| update_worker_filter_stats                                                  True |\n",
            "| use_worker_filter_stats                                                     True |\n",
            "| v_max                                                                        10. |\n",
            "| v_min                                                                       -10. |\n",
            "| validate_env_runners_after_construction                                     True |\n",
            "| worker_cls                                                                    -1 |\n",
            "+----------------------------------------------------------------------------------+\n",
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00001 errored after 0 iterations at 2024-05-12 13:21:12. Total running time: 20s\n",
            "Error file: /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00001_1_lr_schedule=1000000_6e-06,fcnet_hiddens=64_64_2024-05-12_13-20-52/error.txt\n",
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00000 started with configuration:\n",
            "+----------------------------------------------------------------------------------+\n",
            "| Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00000 config                          |\n",
            "+----------------------------------------------------------------------------------+\n",
            "| _AlgorithmConfig__prior_exploration_config                                       |\n",
            "| _disable_action_flattening                                                 False |\n",
            "| _disable_execution_plan_api                                                   -1 |\n",
            "| _disable_initialize_loss_from_dummy_batch                                  False |\n",
            "| _disable_preprocessor_api                                                  False |\n",
            "| _enable_rl_module_api                                                         -1 |\n",
            "| _env_to_module_connector                                                         |\n",
            "| _evaluation_parallel_to_training_wo_thread                                 False |\n",
            "| _fake_gpus                                                                 False |\n",
            "| _is_atari                                                                        |\n",
            "| _learner_class                                                                   |\n",
            "| _learner_connector                                                               |\n",
            "| _module_to_env_connector                                                         |\n",
            "| _rl_module_spec                                                                  |\n",
            "| _run_training_always_in_thread                                             False |\n",
            "| _tf_policy_handles_more_than_one_loss                                      False |\n",
            "| action_mask_key                                                      action_mask |\n",
            "| action_space                                                        Discrete(18) |\n",
            "| actions_in_input_normalized                                                False |\n",
            "| adam_epsilon                                                             0.00015 |\n",
            "| add_default_connectors_to_env_to_module_pipeline                            True |\n",
            "| add_default_connectors_to_learner_pipeline                                  True |\n",
            "| add_default_connectors_to_module_to_env_pipeline                            True |\n",
            "| always_attach_evaluation_results                                            True |\n",
            "| auto_wrap_old_gym_envs                                                        -1 |\n",
            "| batch_mode                                                     complete_episodes |\n",
            "| before_learn_on_batch                                                            |\n",
            "| callbacks                                                   ...efaultCallbacks'> |\n",
            "| categorical_distribution_temperature                                          1. |\n",
            "| checkpoint_trainable_policies_only                                         False |\n",
            "| clip_actions                                                               False |\n",
            "| clip_rewards                                                                     |\n",
            "| compress_observations                                                      False |\n",
            "| count_steps_by                                                         env_steps |\n",
            "| create_env_on_driver                                                       False |\n",
            "| custom_async_evaluation_function                                              -1 |\n",
            "| custom_eval_function                                                             |\n",
            "| delay_between_env_runner_restarts_s                                          60. |\n",
            "| disable_env_checking                                                          -1 |\n",
            "| double_q                                                                    True |\n",
            "| dueling                                                                     True |\n",
            "| eager_max_retraces                                                            20 |\n",
            "| eager_tracing                                                               True |\n",
            "| enable_async_evaluation                                                       -1 |\n",
            "| enable_connectors                                                           True |\n",
            "| enable_env_runner_and_connector_v2                                         False |\n",
            "| enable_rl_module_and_learner                                               False |\n",
            "| enable_tf1_exec_eagerly                                                    False |\n",
            "| env                                                             ALE/BankHeist-v5 |\n",
            "| env_runner_cls                                                                   |\n",
            "| env_runner_health_probe_timeout_s                                             30 |\n",
            "| env_runner_restore_timeout_s                                                1800 |\n",
            "| env_task_fn                                                                      |\n",
            "| episode_lookback_horizon                                                       1 |\n",
            "| epsilon                                                     ...), (10000, 0.05)] |\n",
            "| evaluation_config/explore                                                  False |\n",
            "| evaluation_duration                                                           10 |\n",
            "| evaluation_duration_unit                                                episodes |\n",
            "| evaluation_force_reset_envs_before_iteration                                True |\n",
            "| evaluation_interval                                                              |\n",
            "| evaluation_num_env_runners                                                     0 |\n",
            "| evaluation_parallel_to_training                                            False |\n",
            "| evaluation_sample_timeout_s                                                 120. |\n",
            "| exploration_config/epsilon_timesteps                                       10000 |\n",
            "| exploration_config/final_epsilon                                            0.02 |\n",
            "| exploration_config/initial_epsilon                                           1.0 |\n",
            "| exploration_config/type                                            EpsilonGreedy |\n",
            "| explore                                                                     True |\n",
            "| export_native_model_files                                                  False |\n",
            "| fake_sampler                                                               False |\n",
            "| framework                                                                  torch |\n",
            "| gamma                                                                       0.99 |\n",
            "| grad_clip                                                                     10 |\n",
            "| grad_clip_by                                                         global_norm |\n",
            "| hiddens                                                                    [256] |\n",
            "| ignore_env_runner_failures                                                 False |\n",
            "| in_evaluation                                                              False |\n",
            "| input                                                                    sampler |\n",
            "| keep_per_episode_custom_metrics                                            False |\n",
            "| local_gpu_idx                                                                  0 |\n",
            "| local_tf_session_args/inter_op_parallelism_threads                             8 |\n",
            "| local_tf_session_args/intra_op_parallelism_threads                             8 |\n",
            "| log_level                                                                   WARN |\n",
            "| log_sys_usage                                                               True |\n",
            "| logger_config                                                                    |\n",
            "| logger_creator                                                                   |\n",
            "| lr                                                                        0.0005 |\n",
            "| lr_schedule                                                        [0, 6.25e-05] |\n",
            "| max_num_env_runner_restarts                                                 1000 |\n",
            "| max_requests_in_flight_per_sampler_worker                                      2 |\n",
            "| metrics_episode_collection_timeout_s                                         60. |\n",
            "| metrics_num_episodes_for_smoothing                                           100 |\n",
            "| min_sample_timesteps_per_iteration                                          1000 |\n",
            "| min_time_s_per_iteration                                                         |\n",
            "| min_train_timesteps_per_iteration                                              0 |\n",
            "| model/conv_activation                                                       relu |\n",
            "| model/conv_filters                                          ..., 2], [64, 3, 1]] |\n",
            "| model/custom_model                                                    ddqn_model |\n",
            "| model/custom_model_config/encoder_latent_dim                                 128 |\n",
            "| model/fcnet_activation                                                      relu |\n",
            "| model/fcnet_hiddens                                                     [64, 64] |\n",
            "| n_step                                                                         1 |\n",
            "| noisy                                                                      False |\n",
            "| normalize_actions                                                           True |\n",
            "| num_atoms                                                                      1 |\n",
            "| num_consecutive_env_runner_failures_tolerance                                100 |\n",
            "| num_cpus_for_driver                                                            1 |\n",
            "| num_cpus_per_learner_worker                                                    1 |\n",
            "| num_cpus_per_worker                                                            1 |\n",
            "| num_envs_per_env_runner                                                        1 |\n",
            "| num_gpus                                                                       0 |\n",
            "| num_gpus_per_learner_worker                                                    0 |\n",
            "| num_gpus_per_worker                                                            0 |\n",
            "| num_learner_workers                                                            0 |\n",
            "| num_steps_sampled_before_learning_starts                                    1000 |\n",
            "| num_workers                                                                    1 |\n",
            "| observation_filter                                                      NoFilter |\n",
            "| observation_fn                                                                   |\n",
            "| observation_space                                           ..., 160, 3), uint8) |\n",
            "| offline_sampling                                                           False |\n",
            "| ope_split_batch_by_episode                                                  True |\n",
            "| output                                                                           |\n",
            "| output_compress_columns                                       ['obs', 'new_obs'] |\n",
            "| output_max_file_size                                                    67108864 |\n",
            "| placement_strategy                                                          PACK |\n",
            "| policies/default_policy                                     ...None, None, None) |\n",
            "| policies_to_train                                                                |\n",
            "| policy_map_cache                                                              -1 |\n",
            "| policy_map_capacity                                                          100 |\n",
            "| policy_mapping_fn                                           ...t 0x7baee9b49e10> |\n",
            "| policy_states_are_swappable                                                False |\n",
            "| postprocess_inputs                                                         False |\n",
            "| preprocessor_pref                                                       deepmind |\n",
            "| recreate_failed_env_runners                                                False |\n",
            "| remote_env_batch_wait_ms                                                       0 |\n",
            "| remote_worker_envs                                                         False |\n",
            "| render_env                                                                 False |\n",
            "| replay_buffer_config/capacity                                              60000 |\n",
            "| replay_buffer_config/prioritized_replay                                       -1 |\n",
            "| replay_buffer_config/prioritized_replay_alpha                                0.5 |\n",
            "| replay_buffer_config/prioritized_replay_beta                                 0.5 |\n",
            "| replay_buffer_config/prioritized_replay_eps                                3e-06 |\n",
            "| replay_buffer_config/replay_sequence_length                                    1 |\n",
            "| replay_buffer_config/type                                   ...tizedReplayBuffer |\n",
            "| replay_buffer_config/worker_side_prioritization                            False |\n",
            "| replay_sequence_length                                                           |\n",
            "| restart_failed_sub_environments                                            False |\n",
            "| rollout_fragment_length                                                        4 |\n",
            "| sample_collector                                            ...leListCollector'> |\n",
            "| sample_timeout_s                                                             60. |\n",
            "| sampler_perf_stats_ema_coef                                                      |\n",
            "| seed                                                                             |\n",
            "| shuffle_buffer_size                                                            0 |\n",
            "| sigma0                                                                       0.5 |\n",
            "| simple_optimizer                                                              -1 |\n",
            "| store_buffer_in_checkpoints                                                False |\n",
            "| sync_filters_on_rollout_workers_timeout_s                                    10. |\n",
            "| synchronize_filters                                                           -1 |\n",
            "| target_network_update_freq                                                  8000 |\n",
            "| tau                                                                           1. |\n",
            "| td_error_loss_fn                                                           huber |\n",
            "| tf_session_args/allow_soft_placement                                        True |\n",
            "| tf_session_args/device_count/CPU                                               1 |\n",
            "| tf_session_args/gpu_options/allow_growth                                    True |\n",
            "| tf_session_args/inter_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/intra_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/log_device_placement                                       False |\n",
            "| torch_compile_learner                                                      False |\n",
            "| torch_compile_learner_dynamo_backend                                    inductor |\n",
            "| torch_compile_learner_dynamo_mode                                                |\n",
            "| torch_compile_learner_what_to_compile                       ...ile.FORWARD_TRAIN |\n",
            "| torch_compile_worker                                                       False |\n",
            "| torch_compile_worker_dynamo_backend                                       onnxrt |\n",
            "| torch_compile_worker_dynamo_mode                                                 |\n",
            "| train_batch_size                                                              32 |\n",
            "| train_batch_size_per_learner                                                     |\n",
            "| training_intensity                                                               |\n",
            "| update_worker_filter_stats                                                  True |\n",
            "| use_worker_filter_stats                                                     True |\n",
            "| v_max                                                                        10. |\n",
            "| v_min                                                                       -10. |\n",
            "| validate_env_runners_after_construction                                     True |\n",
            "| worker_cls                                                                    -1 |\n",
            "+----------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-12 13:21:12,755\tERROR tune_controller.py:1331 -- Trial task failed for trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2623, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(NotImplementedError): \u001b[36mray::DDQNTrainable.train()\u001b[39m (pid=26703, ip=172.28.0.12, actor_id=1c1b66ae7447114f80c4dddd01000000, repr=<__main__.DDQNTrainable object at 0x7e26de993610>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 328, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 850, in step\n",
            "    raise NotImplementedError\n",
            "NotImplementedError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00000 errored after 0 iterations at 2024-05-12 13:21:12. Total running time: 20s\n",
            "Error file: /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00000_0_lr_schedule=0_6_25e-05,fcnet_hiddens=64_64_2024-05-12_13-20-52/error.txt\n",
            "\n",
            "Trial status: 2 ERROR | 2 PENDING\n",
            "Current time: 2024-05-12 13:21:22. Total running time: 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "+--------------------------------------------------------------------------------------------------+\n",
            "| Trial name                                   status     lr_schedule        model/fcnet_hiddens   |\n",
            "+--------------------------------------------------------------------------------------------------+\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00002   PENDING    [0, 6.25e-05]      [128, 128]            |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00003   PENDING    [1000000, 6e-06]   [128, 128]            |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00000   ERROR      [0, 6.25e-05]      [64, 64]              |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00001   ERROR      [1000000, 6e-06]   [64, 64]              |\n",
            "+--------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=26921)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=26921)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(DDQNTrainable pid=26703)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "\u001b[36m(DDQNTrainable pid=26703)\u001b[0m [Powered by Stella]\n",
            "\u001b[36m(DDQNTrainable pid=26703)\u001b[0m Install gputil for GPU system monitoring.\n",
            "\u001b[36m(DDQNTrainable pid=26921)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "\u001b[36m(DDQNTrainable pid=26921)\u001b[0m [Powered by Stella]\n",
            "\u001b[36m(DDQNTrainable pid=26921)\u001b[0m Install gputil for GPU system monitoring.\n",
            "2024-05-12 13:21:33,030\tERROR tune_controller.py:1331 -- Trial task failed for trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00003\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2623, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(NotImplementedError): \u001b[36mray::DDQNTrainable.train()\u001b[39m (pid=26921, ip=172.28.0.12, actor_id=5c12829d09f77325dae3076a01000000, repr=<__main__.DDQNTrainable object at 0x7a799900f490>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 328, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 850, in step\n",
            "    raise NotImplementedError\n",
            "NotImplementedError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00003 started with configuration:\n",
            "+----------------------------------------------------------------------------------+\n",
            "| Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00003 config                          |\n",
            "+----------------------------------------------------------------------------------+\n",
            "| _AlgorithmConfig__prior_exploration_config                                       |\n",
            "| _disable_action_flattening                                                 False |\n",
            "| _disable_execution_plan_api                                                   -1 |\n",
            "| _disable_initialize_loss_from_dummy_batch                                  False |\n",
            "| _disable_preprocessor_api                                                  False |\n",
            "| _enable_rl_module_api                                                         -1 |\n",
            "| _env_to_module_connector                                                         |\n",
            "| _evaluation_parallel_to_training_wo_thread                                 False |\n",
            "| _fake_gpus                                                                 False |\n",
            "| _is_atari                                                                        |\n",
            "| _learner_class                                                                   |\n",
            "| _learner_connector                                                               |\n",
            "| _module_to_env_connector                                                         |\n",
            "| _rl_module_spec                                                                  |\n",
            "| _run_training_always_in_thread                                             False |\n",
            "| _tf_policy_handles_more_than_one_loss                                      False |\n",
            "| action_mask_key                                                      action_mask |\n",
            "| action_space                                                        Discrete(18) |\n",
            "| actions_in_input_normalized                                                False |\n",
            "| adam_epsilon                                                             0.00015 |\n",
            "| add_default_connectors_to_env_to_module_pipeline                            True |\n",
            "| add_default_connectors_to_learner_pipeline                                  True |\n",
            "| add_default_connectors_to_module_to_env_pipeline                            True |\n",
            "| always_attach_evaluation_results                                            True |\n",
            "| auto_wrap_old_gym_envs                                                        -1 |\n",
            "| batch_mode                                                     complete_episodes |\n",
            "| before_learn_on_batch                                                            |\n",
            "| callbacks                                                   ...efaultCallbacks'> |\n",
            "| categorical_distribution_temperature                                          1. |\n",
            "| checkpoint_trainable_policies_only                                         False |\n",
            "| clip_actions                                                               False |\n",
            "| clip_rewards                                                                     |\n",
            "| compress_observations                                                      False |\n",
            "| count_steps_by                                                         env_steps |\n",
            "| create_env_on_driver                                                       False |\n",
            "| custom_async_evaluation_function                                              -1 |\n",
            "| custom_eval_function                                                             |\n",
            "| delay_between_env_runner_restarts_s                                          60. |\n",
            "| disable_env_checking                                                          -1 |\n",
            "| double_q                                                                    True |\n",
            "| dueling                                                                     True |\n",
            "| eager_max_retraces                                                            20 |\n",
            "| eager_tracing                                                               True |\n",
            "| enable_async_evaluation                                                       -1 |\n",
            "| enable_connectors                                                           True |\n",
            "| enable_env_runner_and_connector_v2                                         False |\n",
            "| enable_rl_module_and_learner                                               False |\n",
            "| enable_tf1_exec_eagerly                                                    False |\n",
            "| env                                                             ALE/BankHeist-v5 |\n",
            "| env_runner_cls                                                                   |\n",
            "| env_runner_health_probe_timeout_s                                             30 |\n",
            "| env_runner_restore_timeout_s                                                1800 |\n",
            "| env_task_fn                                                                      |\n",
            "| episode_lookback_horizon                                                       1 |\n",
            "| epsilon                                                     ...), (10000, 0.05)] |\n",
            "| evaluation_config/explore                                                  False |\n",
            "| evaluation_duration                                                           10 |\n",
            "| evaluation_duration_unit                                                episodes |\n",
            "| evaluation_force_reset_envs_before_iteration                                True |\n",
            "| evaluation_interval                                                              |\n",
            "| evaluation_num_env_runners                                                     0 |\n",
            "| evaluation_parallel_to_training                                            False |\n",
            "| evaluation_sample_timeout_s                                                 120. |\n",
            "| exploration_config/epsilon_timesteps                                       10000 |\n",
            "| exploration_config/final_epsilon                                            0.02 |\n",
            "| exploration_config/initial_epsilon                                           1.0 |\n",
            "| exploration_config/type                                            EpsilonGreedy |\n",
            "| explore                                                                     True |\n",
            "| export_native_model_files                                                  False |\n",
            "| fake_sampler                                                               False |\n",
            "| framework                                                                  torch |\n",
            "| gamma                                                                       0.99 |\n",
            "| grad_clip                                                                     10 |\n",
            "| grad_clip_by                                                         global_norm |\n",
            "| hiddens                                                                    [256] |\n",
            "| ignore_env_runner_failures                                                 False |\n",
            "| in_evaluation                                                              False |\n",
            "| input                                                                    sampler |\n",
            "| keep_per_episode_custom_metrics                                            False |\n",
            "| local_gpu_idx                                                                  0 |\n",
            "| local_tf_session_args/inter_op_parallelism_threads                             8 |\n",
            "| local_tf_session_args/intra_op_parallelism_threads                             8 |\n",
            "| log_level                                                                   WARN |\n",
            "| log_sys_usage                                                               True |\n",
            "| logger_config                                                                    |\n",
            "| logger_creator                                                                   |\n",
            "| lr                                                                        0.0005 |\n",
            "| lr_schedule                                                     [1000000, 6e-06] |\n",
            "| max_num_env_runner_restarts                                                 1000 |\n",
            "| max_requests_in_flight_per_sampler_worker                                      2 |\n",
            "| metrics_episode_collection_timeout_s                                         60. |\n",
            "| metrics_num_episodes_for_smoothing                                           100 |\n",
            "| min_sample_timesteps_per_iteration                                          1000 |\n",
            "| min_time_s_per_iteration                                                         |\n",
            "| min_train_timesteps_per_iteration                                              0 |\n",
            "| model/conv_activation                                                       relu |\n",
            "| model/conv_filters                                          ..., 2], [64, 3, 1]] |\n",
            "| model/custom_model                                                    ddqn_model |\n",
            "| model/custom_model_config/encoder_latent_dim                                 128 |\n",
            "| model/fcnet_activation                                                      relu |\n",
            "| model/fcnet_hiddens                                                   [128, 128] |\n",
            "| n_step                                                                         1 |\n",
            "| noisy                                                                      False |\n",
            "| normalize_actions                                                           True |\n",
            "| num_atoms                                                                      1 |\n",
            "| num_consecutive_env_runner_failures_tolerance                                100 |\n",
            "| num_cpus_for_driver                                                            1 |\n",
            "| num_cpus_per_learner_worker                                                    1 |\n",
            "| num_cpus_per_worker                                                            1 |\n",
            "| num_envs_per_env_runner                                                        1 |\n",
            "| num_gpus                                                                       0 |\n",
            "| num_gpus_per_learner_worker                                                    0 |\n",
            "| num_gpus_per_worker                                                            0 |\n",
            "| num_learner_workers                                                            0 |\n",
            "| num_steps_sampled_before_learning_starts                                    1000 |\n",
            "| num_workers                                                                    1 |\n",
            "| observation_filter                                                      NoFilter |\n",
            "| observation_fn                                                                   |\n",
            "| observation_space                                           ..., 160, 3), uint8) |\n",
            "| offline_sampling                                                           False |\n",
            "| ope_split_batch_by_episode                                                  True |\n",
            "| output                                                                           |\n",
            "| output_compress_columns                                       ['obs', 'new_obs'] |\n",
            "| output_max_file_size                                                    67108864 |\n",
            "| placement_strategy                                                          PACK |\n",
            "| policies/default_policy                                     ...None, None, None) |\n",
            "| policies_to_train                                                                |\n",
            "| policy_map_cache                                                              -1 |\n",
            "| policy_map_capacity                                                          100 |\n",
            "| policy_mapping_fn                                           ...t 0x7baee9b49e10> |\n",
            "| policy_states_are_swappable                                                False |\n",
            "| postprocess_inputs                                                         False |\n",
            "| preprocessor_pref                                                       deepmind |\n",
            "| recreate_failed_env_runners                                                False |\n",
            "| remote_env_batch_wait_ms                                                       0 |\n",
            "| remote_worker_envs                                                         False |\n",
            "| render_env                                                                 False |\n",
            "| replay_buffer_config/capacity                                              60000 |\n",
            "| replay_buffer_config/prioritized_replay                                       -1 |\n",
            "| replay_buffer_config/prioritized_replay_alpha                                0.5 |\n",
            "| replay_buffer_config/prioritized_replay_beta                                 0.5 |\n",
            "| replay_buffer_config/prioritized_replay_eps                                3e-06 |\n",
            "| replay_buffer_config/replay_sequence_length                                    1 |\n",
            "| replay_buffer_config/type                                   ...tizedReplayBuffer |\n",
            "| replay_buffer_config/worker_side_prioritization                            False |\n",
            "| replay_sequence_length                                                           |\n",
            "| restart_failed_sub_environments                                            False |\n",
            "| rollout_fragment_length                                                        4 |\n",
            "| sample_collector                                            ...leListCollector'> |\n",
            "| sample_timeout_s                                                             60. |\n",
            "| sampler_perf_stats_ema_coef                                                      |\n",
            "| seed                                                                             |\n",
            "| shuffle_buffer_size                                                            0 |\n",
            "| sigma0                                                                       0.5 |\n",
            "| simple_optimizer                                                              -1 |\n",
            "| store_buffer_in_checkpoints                                                False |\n",
            "| sync_filters_on_rollout_workers_timeout_s                                    10. |\n",
            "| synchronize_filters                                                           -1 |\n",
            "| target_network_update_freq                                                  8000 |\n",
            "| tau                                                                           1. |\n",
            "| td_error_loss_fn                                                           huber |\n",
            "| tf_session_args/allow_soft_placement                                        True |\n",
            "| tf_session_args/device_count/CPU                                               1 |\n",
            "| tf_session_args/gpu_options/allow_growth                                    True |\n",
            "| tf_session_args/inter_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/intra_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/log_device_placement                                       False |\n",
            "| torch_compile_learner                                                      False |\n",
            "| torch_compile_learner_dynamo_backend                                    inductor |\n",
            "| torch_compile_learner_dynamo_mode                                                |\n",
            "| torch_compile_learner_what_to_compile                       ...ile.FORWARD_TRAIN |\n",
            "| torch_compile_worker                                                       False |\n",
            "| torch_compile_worker_dynamo_backend                                       onnxrt |\n",
            "| torch_compile_worker_dynamo_mode                                                 |\n",
            "| train_batch_size                                                              32 |\n",
            "| train_batch_size_per_learner                                                     |\n",
            "| training_intensity                                                               |\n",
            "| update_worker_filter_stats                                                  True |\n",
            "| use_worker_filter_stats                                                     True |\n",
            "| v_max                                                                        10. |\n",
            "| v_min                                                                       -10. |\n",
            "| validate_env_runners_after_construction                                     True |\n",
            "| worker_cls                                                                    -1 |\n",
            "+----------------------------------------------------------------------------------+\n",
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00003 errored after 0 iterations at 2024-05-12 13:21:33. Total running time: 40s\n",
            "Error file: /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00003_3_lr_schedule=1000000_6e-06,fcnet_hiddens=128_128_2024-05-12_13-20-52/error.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-12 13:21:33,327\tERROR tune_controller.py:1331 -- Trial task failed for trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00002\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2623, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(NotImplementedError): \u001b[36mray::DDQNTrainable.train()\u001b[39m (pid=26915, ip=172.28.0.12, actor_id=7f05756c0b86c7431ef5a13001000000, repr=<__main__.DDQNTrainable object at 0x7a7797187310>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 328, in train\n",
            "    result = self.step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 850, in step\n",
            "    raise NotImplementedError\n",
            "NotImplementedError\n",
            "2024-05-12 13:21:33,370\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
            "2024-05-12 13:21:33,387\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/DDQNTrainable_2024-05-12_13-20-52' in 0.0499s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00002 started with configuration:\n",
            "+----------------------------------------------------------------------------------+\n",
            "| Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00002 config                          |\n",
            "+----------------------------------------------------------------------------------+\n",
            "| _AlgorithmConfig__prior_exploration_config                                       |\n",
            "| _disable_action_flattening                                                 False |\n",
            "| _disable_execution_plan_api                                                   -1 |\n",
            "| _disable_initialize_loss_from_dummy_batch                                  False |\n",
            "| _disable_preprocessor_api                                                  False |\n",
            "| _enable_rl_module_api                                                         -1 |\n",
            "| _env_to_module_connector                                                         |\n",
            "| _evaluation_parallel_to_training_wo_thread                                 False |\n",
            "| _fake_gpus                                                                 False |\n",
            "| _is_atari                                                                        |\n",
            "| _learner_class                                                                   |\n",
            "| _learner_connector                                                               |\n",
            "| _module_to_env_connector                                                         |\n",
            "| _rl_module_spec                                                                  |\n",
            "| _run_training_always_in_thread                                             False |\n",
            "| _tf_policy_handles_more_than_one_loss                                      False |\n",
            "| action_mask_key                                                      action_mask |\n",
            "| action_space                                                        Discrete(18) |\n",
            "| actions_in_input_normalized                                                False |\n",
            "| adam_epsilon                                                             0.00015 |\n",
            "| add_default_connectors_to_env_to_module_pipeline                            True |\n",
            "| add_default_connectors_to_learner_pipeline                                  True |\n",
            "| add_default_connectors_to_module_to_env_pipeline                            True |\n",
            "| always_attach_evaluation_results                                            True |\n",
            "| auto_wrap_old_gym_envs                                                        -1 |\n",
            "| batch_mode                                                     complete_episodes |\n",
            "| before_learn_on_batch                                                            |\n",
            "| callbacks                                                   ...efaultCallbacks'> |\n",
            "| categorical_distribution_temperature                                          1. |\n",
            "| checkpoint_trainable_policies_only                                         False |\n",
            "| clip_actions                                                               False |\n",
            "| clip_rewards                                                                     |\n",
            "| compress_observations                                                      False |\n",
            "| count_steps_by                                                         env_steps |\n",
            "| create_env_on_driver                                                       False |\n",
            "| custom_async_evaluation_function                                              -1 |\n",
            "| custom_eval_function                                                             |\n",
            "| delay_between_env_runner_restarts_s                                          60. |\n",
            "| disable_env_checking                                                          -1 |\n",
            "| double_q                                                                    True |\n",
            "| dueling                                                                     True |\n",
            "| eager_max_retraces                                                            20 |\n",
            "| eager_tracing                                                               True |\n",
            "| enable_async_evaluation                                                       -1 |\n",
            "| enable_connectors                                                           True |\n",
            "| enable_env_runner_and_connector_v2                                         False |\n",
            "| enable_rl_module_and_learner                                               False |\n",
            "| enable_tf1_exec_eagerly                                                    False |\n",
            "| env                                                             ALE/BankHeist-v5 |\n",
            "| env_runner_cls                                                                   |\n",
            "| env_runner_health_probe_timeout_s                                             30 |\n",
            "| env_runner_restore_timeout_s                                                1800 |\n",
            "| env_task_fn                                                                      |\n",
            "| episode_lookback_horizon                                                       1 |\n",
            "| epsilon                                                     ...), (10000, 0.05)] |\n",
            "| evaluation_config/explore                                                  False |\n",
            "| evaluation_duration                                                           10 |\n",
            "| evaluation_duration_unit                                                episodes |\n",
            "| evaluation_force_reset_envs_before_iteration                                True |\n",
            "| evaluation_interval                                                              |\n",
            "| evaluation_num_env_runners                                                     0 |\n",
            "| evaluation_parallel_to_training                                            False |\n",
            "| evaluation_sample_timeout_s                                                 120. |\n",
            "| exploration_config/epsilon_timesteps                                       10000 |\n",
            "| exploration_config/final_epsilon                                            0.02 |\n",
            "| exploration_config/initial_epsilon                                           1.0 |\n",
            "| exploration_config/type                                            EpsilonGreedy |\n",
            "| explore                                                                     True |\n",
            "| export_native_model_files                                                  False |\n",
            "| fake_sampler                                                               False |\n",
            "| framework                                                                  torch |\n",
            "| gamma                                                                       0.99 |\n",
            "| grad_clip                                                                     10 |\n",
            "| grad_clip_by                                                         global_norm |\n",
            "| hiddens                                                                    [256] |\n",
            "| ignore_env_runner_failures                                                 False |\n",
            "| in_evaluation                                                              False |\n",
            "| input                                                                    sampler |\n",
            "| keep_per_episode_custom_metrics                                            False |\n",
            "| local_gpu_idx                                                                  0 |\n",
            "| local_tf_session_args/inter_op_parallelism_threads                             8 |\n",
            "| local_tf_session_args/intra_op_parallelism_threads                             8 |\n",
            "| log_level                                                                   WARN |\n",
            "| log_sys_usage                                                               True |\n",
            "| logger_config                                                                    |\n",
            "| logger_creator                                                                   |\n",
            "| lr                                                                        0.0005 |\n",
            "| lr_schedule                                                        [0, 6.25e-05] |\n",
            "| max_num_env_runner_restarts                                                 1000 |\n",
            "| max_requests_in_flight_per_sampler_worker                                      2 |\n",
            "| metrics_episode_collection_timeout_s                                         60. |\n",
            "| metrics_num_episodes_for_smoothing                                           100 |\n",
            "| min_sample_timesteps_per_iteration                                          1000 |\n",
            "| min_time_s_per_iteration                                                         |\n",
            "| min_train_timesteps_per_iteration                                              0 |\n",
            "| model/conv_activation                                                       relu |\n",
            "| model/conv_filters                                          ..., 2], [64, 3, 1]] |\n",
            "| model/custom_model                                                    ddqn_model |\n",
            "| model/custom_model_config/encoder_latent_dim                                 128 |\n",
            "| model/fcnet_activation                                                      relu |\n",
            "| model/fcnet_hiddens                                                   [128, 128] |\n",
            "| n_step                                                                         1 |\n",
            "| noisy                                                                      False |\n",
            "| normalize_actions                                                           True |\n",
            "| num_atoms                                                                      1 |\n",
            "| num_consecutive_env_runner_failures_tolerance                                100 |\n",
            "| num_cpus_for_driver                                                            1 |\n",
            "| num_cpus_per_learner_worker                                                    1 |\n",
            "| num_cpus_per_worker                                                            1 |\n",
            "| num_envs_per_env_runner                                                        1 |\n",
            "| num_gpus                                                                       0 |\n",
            "| num_gpus_per_learner_worker                                                    0 |\n",
            "| num_gpus_per_worker                                                            0 |\n",
            "| num_learner_workers                                                            0 |\n",
            "| num_steps_sampled_before_learning_starts                                    1000 |\n",
            "| num_workers                                                                    1 |\n",
            "| observation_filter                                                      NoFilter |\n",
            "| observation_fn                                                                   |\n",
            "| observation_space                                           ..., 160, 3), uint8) |\n",
            "| offline_sampling                                                           False |\n",
            "| ope_split_batch_by_episode                                                  True |\n",
            "| output                                                                           |\n",
            "| output_compress_columns                                       ['obs', 'new_obs'] |\n",
            "| output_max_file_size                                                    67108864 |\n",
            "| placement_strategy                                                          PACK |\n",
            "| policies/default_policy                                     ...None, None, None) |\n",
            "| policies_to_train                                                                |\n",
            "| policy_map_cache                                                              -1 |\n",
            "| policy_map_capacity                                                          100 |\n",
            "| policy_mapping_fn                                           ...t 0x7baee9b49e10> |\n",
            "| policy_states_are_swappable                                                False |\n",
            "| postprocess_inputs                                                         False |\n",
            "| preprocessor_pref                                                       deepmind |\n",
            "| recreate_failed_env_runners                                                False |\n",
            "| remote_env_batch_wait_ms                                                       0 |\n",
            "| remote_worker_envs                                                         False |\n",
            "| render_env                                                                 False |\n",
            "| replay_buffer_config/capacity                                              60000 |\n",
            "| replay_buffer_config/prioritized_replay                                       -1 |\n",
            "| replay_buffer_config/prioritized_replay_alpha                                0.5 |\n",
            "| replay_buffer_config/prioritized_replay_beta                                 0.5 |\n",
            "| replay_buffer_config/prioritized_replay_eps                                3e-06 |\n",
            "| replay_buffer_config/replay_sequence_length                                    1 |\n",
            "| replay_buffer_config/type                                   ...tizedReplayBuffer |\n",
            "| replay_buffer_config/worker_side_prioritization                            False |\n",
            "| replay_sequence_length                                                           |\n",
            "| restart_failed_sub_environments                                            False |\n",
            "| rollout_fragment_length                                                        4 |\n",
            "| sample_collector                                            ...leListCollector'> |\n",
            "| sample_timeout_s                                                             60. |\n",
            "| sampler_perf_stats_ema_coef                                                      |\n",
            "| seed                                                                             |\n",
            "| shuffle_buffer_size                                                            0 |\n",
            "| sigma0                                                                       0.5 |\n",
            "| simple_optimizer                                                              -1 |\n",
            "| store_buffer_in_checkpoints                                                False |\n",
            "| sync_filters_on_rollout_workers_timeout_s                                    10. |\n",
            "| synchronize_filters                                                           -1 |\n",
            "| target_network_update_freq                                                  8000 |\n",
            "| tau                                                                           1. |\n",
            "| td_error_loss_fn                                                           huber |\n",
            "| tf_session_args/allow_soft_placement                                        True |\n",
            "| tf_session_args/device_count/CPU                                               1 |\n",
            "| tf_session_args/gpu_options/allow_growth                                    True |\n",
            "| tf_session_args/inter_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/intra_op_parallelism_threads                                   2 |\n",
            "| tf_session_args/log_device_placement                                       False |\n",
            "| torch_compile_learner                                                      False |\n",
            "| torch_compile_learner_dynamo_backend                                    inductor |\n",
            "| torch_compile_learner_dynamo_mode                                                |\n",
            "| torch_compile_learner_what_to_compile                       ...ile.FORWARD_TRAIN |\n",
            "| torch_compile_worker                                                       False |\n",
            "| torch_compile_worker_dynamo_backend                                       onnxrt |\n",
            "| torch_compile_worker_dynamo_mode                                                 |\n",
            "| train_batch_size                                                              32 |\n",
            "| train_batch_size_per_learner                                                     |\n",
            "| training_intensity                                                               |\n",
            "| update_worker_filter_stats                                                  True |\n",
            "| use_worker_filter_stats                                                     True |\n",
            "| v_max                                                                        10. |\n",
            "| v_min                                                                       -10. |\n",
            "| validate_env_runners_after_construction                                     True |\n",
            "| worker_cls                                                                    -1 |\n",
            "+----------------------------------------------------------------------------------+\n",
            "\n",
            "Trial DDQNTrainable_ALE_BankHeist-v5_74bbd_00002 errored after 0 iterations at 2024-05-12 13:21:33. Total running time: 41s\n",
            "Error file: /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00002_2_lr_schedule=0_6_25e-05,fcnet_hiddens=128_128_2024-05-12_13-20-52/error.txt\n",
            "\n",
            "Trial status: 4 ERROR\n",
            "Current time: 2024-05-12 13:21:33. Total running time: 41s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "+--------------------------------------------------------------------------------------------------+\n",
            "| Trial name                                   status     lr_schedule        model/fcnet_hiddens   |\n",
            "+--------------------------------------------------------------------------------------------------+\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00000   ERROR      [0, 6.25e-05]      [64, 64]              |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00001   ERROR      [1000000, 6e-06]   [64, 64]              |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00002   ERROR      [0, 6.25e-05]      [128, 128]            |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00003   ERROR      [1000000, 6e-06]   [128, 128]            |\n",
            "+--------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Number of errored trials: 4\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                                     # failures   error file                                                                                                                                                                                                                                                  |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00000              1   /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00000_0_lr_schedule=0_6_25e-05,fcnet_hiddens=64_64_2024-05-12_13-20-52/error.txt      |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00001              1   /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00001_1_lr_schedule=1000000_6e-06,fcnet_hiddens=64_64_2024-05-12_13-20-52/error.txt   |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00002              1   /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00002_2_lr_schedule=0_6_25e-05,fcnet_hiddens=128_128_2024-05-12_13-20-52/error.txt    |\n",
            "| DDQNTrainable_ALE_BankHeist-v5_74bbd_00003              1   /tmp/ray/session_2024-05-12_12-52-10_506427_241/artifacts/2024-05-12_13-20-52/DDQNTrainable_2024-05-12_13-20-52/driver_artifacts/DDQNTrainable_ALE_BankHeist-v5_74bbd_00003_3_lr_schedule=1000000_6e-06,fcnet_hiddens=128_128_2024-05-12_13-20-52/error.txt |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-05-12 13:21:34,003\tERROR tune.py:1035 -- Trials did not complete: [DDQNTrainable_ALE_BankHeist-v5_74bbd_00000, DDQNTrainable_ALE_BankHeist-v5_74bbd_00001, DDQNTrainable_ALE_BankHeist-v5_74bbd_00002, DDQNTrainable_ALE_BankHeist-v5_74bbd_00003]\n",
            "2024-05-12 13:21:34,023\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No best trial found for the given metric: episode_reward_mean. This means that no trial has reported this metric.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5dc62cf4ca71>\u001b[0m in \u001b[0;36m<cell line: 234>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;31m# Get the best result from the tuning process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0mbest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"episode_reward_mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_nan_and_inf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;31m# Restore the best model checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/result_grid.py\u001b[0m in \u001b[0;36mget_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: episode_reward_mean. This means that no trial has reported this metric."
          ]
        }
      ]
    }
  ]
}